import { Page } from 'playwright';
import { SEOCheckResult } from '../types';

export class RobotsTxtChecker {
  constructor(private page: Page) {}

  async checkAll(): Promise<SEOCheckResult[]> {
    const results: SEOCheckResult[] = [];

    results.push(await this.checkRobotsTxtExists());
    results.push(await this.checkRobotsTxtAccessible());

    return results;
  }

  private async checkRobotsTxtExists(): Promise<SEOCheckResult> {
    try {
      const url = new URL(this.page.url());
      const robotsUrl = `${url.protocol}//${url.host}/robots.txt`;

      const response = await this.page.context().request.get(robotsUrl);
      const status = response.status();

      if (status === 200) {
        const content = await response.text();
        const hasUserAgent = content.toLowerCase().includes('user-agent:');
        const hasDisallow = content.toLowerCase().includes('disallow:');

        return {
          passed: true,
          message: 'robots.txt file exists and is accessible',
          details: {
            url: robotsUrl,
            status,
            hasUserAgent,
            hasDisallow,
            size: content.length,
          },
        };
      } else if (status === 404) {
        return {
          passed: false,
          message: 'robots.txt file not found (404)',
          details: { url: robotsUrl, status },
        };
      } else {
        return {
          passed: false,
          message: `robots.txt returned unexpected status: ${status}`,
          details: { url: robotsUrl, status },
        };
      }
    } catch (error) {
      return {
        passed: false,
        message: `Error checking robots.txt: ${error instanceof Error ? error.message : 'Unknown error'}`,
      };
    }
  }

  private async checkRobotsTxtAccessible(): Promise<SEOCheckResult> {
    try {
      const url = new URL(this.page.url());
      const robotsUrl = `${url.protocol}//${url.host}/robots.txt`;

      const response = await this.page.context().request.get(robotsUrl);
      const content = await response.text();

      if (response.status() !== 200) {
        return {
          passed: true,
          message: 'robots.txt validation skipped (file not found)',
        };
      }

      // Check for common issues
      const issues: string[] = [];

      // Check if robots.txt blocks important resources
      if (content.toLowerCase().includes('disallow: /')) {
        const lines = content.split('\n');
        const disallowAll = lines.some(
          (line: string) =>
            line.trim().toLowerCase() === 'disallow: /' &&
            !line.trim().startsWith('#')
        );
        if (disallowAll) {
          issues.push('Warning: robots.txt contains "Disallow: /" which blocks all crawlers');
        }
      }

      // Check for sitemap reference
      const hasSitemapReference = content.toLowerCase().includes('sitemap:');

      if (!hasSitemapReference) {
        issues.push('Tip: Consider adding sitemap reference to robots.txt');
      }

      return {
        passed: issues.length === 0,
        message:
          issues.length === 0
            ? 'robots.txt is properly configured'
            : 'robots.txt has potential issues',
        details: {
          issues,
          hasSitemapReference,
          content: content.substring(0, 500), // First 500 chars
        },
      };
    } catch (error) {
      return {
        passed: true,
        message: 'robots.txt validation skipped due to error',
      };
    }
  }
}
